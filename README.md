# image-explain-api

**Upload any image â†’ GPT-4o-mini provides intelligent AI-powered explanation!**

---

## âœ¨ Features

- ğŸ–¼ï¸ **Image Upload** - Drag & drop or select images
- ğŸ¤– **AI Vision Analysis** - GPT-4o-mini model for detailed explanations
- âš¡ **Fast Processing** - Real-time response using OpenRouter API
- ğŸ¨ **Clean UI** - Simple and intuitive web interface
- ğŸ”’ **Secure** - API key stored securely in environment variables

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Backend** | FastAPI + Uvicorn |
| **AI Model** | OpenAI GPT-4o-mini (via OpenRouter) |
| **Templates** | Jinja2 HTML |
| **Deployment** | Render.com |
| **Language** | Python 3.9+ |

---

## ğŸ“‹ Prerequisites

```bash
Python 3.9+
pip package manager
OpenRouter API key (free at openrouter.ai)
Local Setup
1ï¸âƒ£ Clone Repository
bash
git clone https://github.com/bandhana498-source/image-explain-api.git
cd image-explain-api
2ï¸âƒ£ Install Dependencies
bash
pip install -r requirements.txt
3ï¸âƒ£ Setup Environment Variables
Create .env file:
Ctext
OPENROUTER_API_KEY=sk-or-v1-your_key_here
Get your free API key: OpenRouter.ai

4ï¸âƒ£ Run Application
bash
python -m uvicorn main:app --reload
Open: http://127.0.0.1:8000
://127.0.0.1:8000

ğŸ“¡ API Endpoints
GET /
Returns HTML upload form

Response: HTML page with image upload form

POST /explain
Analyzes uploaded image using GPT-4o-mini

Request:

text
Content-Type: multipart/form-data
file: <image_file>
Response:

json
{
  "explanation": "The image shows a detailed AI-powered analysis..."
}
ğŸ“ Project Structure
image-explain-api/
â”œâ”€â”€ main.py          # FastAPI app
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ templates/       # HTML templates
â”‚   â””â”€â”€ index.html
â””â”€â”€ README.md

 file
ğŸ”§ requirements.txt
text
fastapi==0.115.0
uvicorn[standard]==0.30.6
python-dotenv==1.0.1
openai==1.51.0
jinja2==3.1.4
python-multipart==0.0.9
ğŸ’» Code Example
python
from fastapi import FastAPI, UploadFile, File
from openai import OpenAI

app = FastAPI()
client = OpenAI(
    api_key=os.getenv("OPENROUTER_API_KEY"),
    base_url="https://openrouter.ai/api/v1"
)

@app.post("/explain")
async def explain_image(file: UploadFile = File(...)):
    image_data = await file.read()
    base64_image = base64.b64encode(image_data).decode('utf-8')
    
    response = client.chat.completions.create(
        model="openai/gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": [{
                "type": "text",
                "text": "Identify and explain this image in detail"
            }, {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
            }]
        }]
    )
    return {"explanation": response.choices.message.content}Usage Example
Open http://localhost:8000

Click "Choose File" and select an image

Click "Explain Image"

AI-powered explanation appears instantly!

ğŸ” Security Notes
âœ… .env file NOT committed to GitHub

âœ… Use environment variables for sensitive data

âœ… API key stored securely on Render dashboard

âœ… CORS enabled for productionTroubleshooting
Issue	Solution
ModuleNotFoundError	Run pip install -r requirements.txt
API key error	Check .env file and OPENROUTER_API_KEY
Port 8000 in use	Change port: uvicorn main:app --port 8001
CORS error	Ensure CORS middleware is enabled in main.py
 Resources
FastAPI Docs

OpenRouter API

Render Deployment

OpenAI Vision

